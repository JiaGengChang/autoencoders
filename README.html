<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jia Geng Chang" />
  <title>Self-supervised learning with autoencoders</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Self-supervised learning with autoencoders</h1>
<p class="author">Jia Geng Chang</p>
<p class="date">December 2021</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#encoding-a-8-bit-one-hot-vector-with-3-bits">Encoding a 8-bit one-hot vector with 3 bits</a></li>
<li><a href="#training-results">Training results</a></li>
<li><a href="#how-8-3-8-autoencoder-learns-input-representation">How 8-3-8 autoencoder learns input representation</a></li>
<li><a href="#encoding-a-16-bit-one-hot-vector-with-3-bits">Encoding a 16-bit one-hot vector with 3 bits</a></li>
<li><a href="#how-16-3-16-autoencoder-learns-input-representation">How 16-3-16 autoencoder learns input representation</a></li>
<li><a href="#code-availability">Code availability</a></li>
</ul>
</nav>
<p>This is a classical autoencoder for intended for CPU training, that is written in optimized code using C.</p>
<h1 id="encoding-a-8-bit-one-hot-vector-with-3-bits">Encoding a 8-bit one-hot vector with 3 bits</h1>
<p>I trained a multi-layer perceptron to encode with 3 units a one-hot encoded vector of length 8. e.g.</p>
<p>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0</p>
<p>represents the decimal digit 7 and</p>
<p>0 &amp; 0 &amp; 0 &amp; 4 &amp; 0 &amp; 0 &amp; 0 &amp; 0</p>
<p>represents the decimal digit 4. Because only one value can be 1, there are only 8 such vectors corresponding to the rows of a 8x8 identity matrix â€” a relatively easy pattern for the network to learn to encode and decode.</p>
<p>Here are the hyperparameters of the autoencoder:</p>
<table>
<caption>Network architecture</caption>
<thead>
<tr class="header">
<th style="text-align: center;">activation function</th>
<th style="text-align: center;">sigmoid</th>
<th style="text-align: center;"><span class="math inline">\(\sigma(x) = \frac{1}{1 + exp(-x}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">loss function</td>
<td style="text-align: center;">binary cross-entropy</td>
<td style="text-align: center;">*equation below</td>
</tr>
<tr class="even">
<td style="text-align: center;">optimizer</td>
<td style="text-align: center;">classical momentum</td>
<td style="text-align: center;">m=0-0.9</td>
</tr>
<tr class="odd">
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">min 0.01</td>
<td style="text-align: center;">max 1.0</td>
</tr>
<tr class="even">
<td style="text-align: center;">weights</td>
<td style="text-align: center;">random uniform initialization</td>
<td style="text-align: center;">Between -0.1 and 0.1</td>
</tr>
</tbody>
</table>
<p><span id="tab:my_label" label="tab:my_label">[tab:my_label]</span></p>
<p>Binary cross-entropy loss is: <span class="math display">\[-\frac{1}{N}\,\sum_{k=1}^{N}\,t_{k}\,log(p_{k}) + (1-t_{k}))\,(log(1-p_{k}))\]</span> where <span class="math inline">\(\mathbf{t}\)</span> is the target one-hot encoded vector, and <span class="math inline">\(\mathbf{p}\)</span> is the output layer activation which is a probability vector, and <span class="math inline">\(N\)</span> is 8 in our case. The derivatives of loss with respect to weights worked out to be: <span class="math display">\[\pdv{E}{W_{jk}} = -\frac{1}{N}\,(\frac{t_{k}}{z_{k}}-\frac{1-t_{k}}{1-z_{k}})\: g&#39;(x_{k})\: z_{j} = \delta_{k}z_{j}\]</span> for the weights between hidden and output units and <span class="math display">\[\pdv{E}{W_{ij}} = -\frac{1}{N} \sum\limits_{k=1}^{N} \: (\frac{t_{k}}{z_{k}}-\frac{1-t_{k}}{1-z_{k}})\: g&#39;(x_{k}) \: w_{jk} \: g&#39;(x_{j}) \: z_{i} = \delta_{j}z_{i}\]</span> for the weights between input and hidden units. The derivative <span class="math inline">\(g&#39;\)</span> of the sigmoid function <span class="math inline">\(g\)</span> is <span class="math inline">\(g&#39; = g(1-g)\)</span>.</p>
<h1 id="training-results">Training results</h1>
<figure>
<img src="mlp_m" alt="With learning rate held constant, momentum speeds up learning" style="width:12cm;height:5cm" /><figcaption>With learning rate held constant, momentum speeds up learning</figcaption>
</figure>
<p><span id="fig:mlp_momentum" label="fig:mlp_momentum">[fig:mlp_momentum]</span></p>
<p>With standard gradient descent, training was slow, even at high learning rates of up to 10. Thus, I added a momentum term involving the previous time step as a modification to vanilla gradient descent. The weight update heuristic at epoch <span class="math inline">\(t\)</span> becomes:</p>
<p><span class="math display">\[\Delta W_{ij}(t) = -\eta \, \pdv{E}{W_{ij}} + m \,\Delta W_{ij}(t-1)\]</span></p>
<p>for a pair of connected neurons <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, and m is scalar that practitioners suggest to be kept between 0 and 1. The result in training with varying <span class="math inline">\(m\)</span>, for two different learning rates (lr = 0.01 and lr = 0.05) is shown in figure <a href="#fig:mlp_momentum" data-reference-type="ref" data-reference="fig:mlp_momentum">[fig:mlp_momentum]</a>.</p>
<p>From <a href="#fig:mlp_momentum" data-reference-type="ref" data-reference="fig:mlp_momentum">[fig:mlp_momentum]</a>, we can see that adding momentum speeds up training greatly. What is happening is that the magnitude of <span class="math inline">\(\Delta W_{ij}(t)\)</span> is increasing by a factor of close to <span class="math inline">\(1+m\)</span> as a function of <span class="math inline">\(t\)</span>, because each time <span class="math inline">\(\nabla E(t)\)</span> is in a similar direction as <span class="math inline">\(\nabla E(t-1)\)</span>.</p>
<h1 id="how-8-3-8-autoencoder-learns-input-representation">How 8-3-8 autoencoder learns input representation</h1>
<p>How is the network learning to encode input vectors? We look at the final state of the weights for the best network to see what is happening.</p>
<figure>
<img src="mlp_w" alt="The network learns to mirror weights between the three layers" style="width:10cm;height:3cm" /><figcaption>The network learns to mirror weights between the three layers</figcaption>
</figure>
<p><span id="fig:mlp_w" label="fig:mlp_w">[fig:mlp_w]</span></p>
<p>Without loss of generality, what the network has learned is to match the signs between mirrored pairs of weights <span class="math inline">\({W_{nj},\; W_{jn} \; \forall \; n \in [0,7], \; j \in [0,2]}\)</span>. For example, if <span class="math inline">\(\mathbf{t}\)</span> is</p>
<p>1 &amp; 0 &amp; ... &amp; 0</p>
<p>, then only <span class="math inline">\(j_0\)</span> and <span class="math inline">\(k_0\)</span> will fire, producing a large positive activation in <span class="math inline">\(k_0\)</span> because <span class="math inline">\(W_{i0,j0}\)</span> and <span class="math inline">\(W_{j0,k0}\)</span> have the same sign (does not matter whether positive or negative). The two other hidden units also have synergistic weights entering/leaving them, but they will stay silent because the inputs into those units is 0.</p>
<h1 id="encoding-a-16-bit-one-hot-vector-with-3-bits">Encoding a 16-bit one-hot vector with 3 bits</h1>
<figure>
<img src="mlp_8_16" alt="The 16-3-16 multi-layer perceptron trains slower across learning rates. Vertical axis is binary cross-entropy loss." style="width:12cm;height:5cm" /><figcaption>The 16-3-16 multi-layer perceptron trains slower across learning rates. Vertical axis is binary cross-entropy loss.</figcaption>
</figure>
<p><span id="fig:mlp_8_16" label="fig:mlp_8_16">[fig:mlp_8_16]</span></p>
<p>Is it possible to be even more efficient and encode a one-hot vector <em>twice</em> the size as our previous vector, with 3 hidden units? If each hidden unit can represent 3 activation states, then we can encode up to a one-hot vector of length <span class="math inline">\(3^n = 3^3 = 27\)</span>. As we are using a sigmoid activation, the three activation states are naturally 0, 0.5, and 1, requiring asymptotic input values of <span class="math inline">\(-\infty\)</span>, 0, and <span class="math inline">\(\infty\)</span> at each hidden unit. While our loss will again never reach 0, it is theoretically possible to train such a network.</p>
<p>In figure <a href="#fig:mlp_8_16" data-reference-type="ref" data-reference="fig:mlp_8_16">[fig:mlp_8_16]</a>, I trained a multi-layer perceptron to encode with 3 units a one-hot encoded vector of length <strong>16</strong>. Training is slower, because there are more weights to adjust, but binary cross-entropy is reduced to a similar extent after around 10000 epochs.</p>
<h1 id="how-16-3-16-autoencoder-learns-input-representation">How 16-3-16 autoencoder learns input representation</h1>
<figure>
<img src="mlp16_w" alt="Weights are again mirrored, and every hidden unit has 3 states - red, white and blue" style="height:3cm" /><figcaption>Weights are again mirrored, and every hidden unit has 3 states - red, white and blue</figcaption>
</figure>
<p><span id="fig:mlp16_w" label="fig:mlp16_w">[fig:mlp16_w]</span></p>
<p>Visualizing the weight matrices in figure <a href="#fig:mlp16_w" data-reference-type="ref" data-reference="fig:mlp16_w">[fig:mlp16_w]</a>, we see that the network is again solving the problem by mirroring the sign of the weights. Additionally, it mirrors the relative magnitude of the weights. This time, each hidden node has three states, as the mirrored incoming and outgoing weights are in one of negative (blue), zero (off-white), and positive (red). The connections entering output node 14 (k14) look all negative, but this is offset by the positive bias (<span class="math inline">\(W_{j3,k14}\)</span>). Again, the bias in the input layer is not doing much (it cannot be any other value than 0 otherwise non-1 units will fire), but the bias in the hidden layer is helping to adjust the median weight to zero.</p>
<h1 id="code-availability">Code availability</h1>
<p>The code for the 8-unit multi-layer perceptron is available as <strong><code>testmlp.c</code></strong>, and the code for the 16-unit multi-layer perceptron is found in <strong><code>testmlp16.c</code></strong>. I also include a <code>utils.c</code> definitions file for simple matrix operations required (e.g., dot product, hadamard product, and scalar operations). The <strong><code>output</code></strong> directory contains the <code>.csv</code> files used to generate the plots. Jupyter notebooks for generating the plots are also included as <code>.ipynb</code> files, and the <code>.png</code> files are found in <code>plots</code>.</p>
</body>
</html>
